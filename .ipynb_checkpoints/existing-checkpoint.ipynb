{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_TRAIN_PATH = './DATA/MiniTrain.csv'\n",
    "TRAIN_PATH = './DATA/TrainData.csv'\n",
    "MINI_TEST_PATH = './DATA/MiniTest.csv'\n",
    "TEST_PATH = './DATA/TestData.csv'\n",
    "HUNDRED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'tech', 'politics', 'sport', 'entertainment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(MINI_TRAIN_PATH)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>One man's claims that he scammed people on the...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>Maury Wills who helped the Los Angeles Dodgers...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>For the past 18 months Hollywood has effective...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>President issues vow as tensions with China rise.</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>An annual celebration took on a different feel...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ArticleId                                               Text       Category\n",
       "0        14  One man's claims that he scammed people on the...           tech\n",
       "1        18  Maury Wills who helped the Los Angeles Dodgers...          sport\n",
       "2        21  For the past 18 months Hollywood has effective...  entertainment\n",
       "3        22  President issues vow as tensions with China rise.       politics\n",
       "4        25  An annual celebration took on a different feel...       politics"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'sport', 'entertainment', 'politics', 'business'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'sport', 'entertainment', 'politics', 'business'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# from nltk.downloader import download, download_shell\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "\n",
    "# Loading stopwords from nltk library\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Function for text preprocessing\n",
    "def txt_preprocessing(total_text, index, column, df):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        # Replace every special character with space\n",
    "        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n",
    "        # Remove multiple spaces\n",
    "        total_text = re.sub('\\s+',' ', total_text)\n",
    "        # Converting to lowercase\n",
    "        total_text = total_text.lower()\n",
    "        \n",
    "        for word in total_text.split():\n",
    "        # If word is not a stopword then retain that word from the data\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        df[column][index] = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>one man claims scammed people platform caused ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>maury wills helped los angeles dodgers win thr...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>past 18 months hollywood effectively boycotted...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>president issues vow tensions china rise</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>annual celebration took different feel russia ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ArticleId                                               Text       Category\n",
       "0        14  one man claims scammed people platform caused ...           tech\n",
       "1        18  maury wills helped los angeles dodgers win thr...          sport\n",
       "2        21  past 18 months hollywood effectively boycotted...  entertainment\n",
       "3        22          president issues vow tensions china rise        politics\n",
       "4        25  annual celebration took different feel russia ...       politics"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in data.iterrows():\n",
    "    if type(row['Text']) is str:\n",
    "        txt_preprocessing(row['Text'], index, 'Text', data)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_data size -> (38153, 3)\n",
      "Testing_data size -> (9539, 3)\n"
     ]
    }
   ],
   "source": [
    "ind = list(data.index)\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "train_len = int(data.shape[0]*0.80)\n",
    "train_ind = ind[:train_len]\n",
    "training_data = data.iloc[train_ind,:]\n",
    "#training_data.head()\n",
    "\n",
    "test_ind = ind[train_len:]\n",
    "testing_data = data.iloc[test_ind,:]\n",
    "#testing_data.head()\n",
    "\n",
    "print('Training_data size -> {}'.format(training_data.shape))\n",
    "print('Testing_data size -> {}'.format(testing_data.shape))\n",
    "\n",
    "assert data.shape[0] ==  len(train_ind)+ len(test_ind), 'Not equal distribution'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB:\n",
    "    def __init__(self, target, dataframe):\n",
    "        self.df = dataframe\n",
    "        # Target/Category Column\n",
    "        self.c_n = target\n",
    "        # Column Names\n",
    "        self.cols = list(self.df.columns)\n",
    "        self.cols.remove(self.c_n)\n",
    "        \n",
    "        self.store = {}\n",
    "        self.likelihood_for_all_()\n",
    "        \n",
    "    def likelihood_cal(self, x, y, z):\n",
    "        \"\"\" \n",
    "        x -> Column Name (String)\n",
    "        y -> Column Value (String)\n",
    "        z -> Class value (String)\n",
    "        c_n -> Class Name (Target)\n",
    "        \n",
    "        Returns -> P(x = y | c_n = z)\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        if x not in self.cols:\n",
    "            raise KeyError(\"Feature(column) not present in the Training Dataset\")\n",
    "        \n",
    "        res =  len(df[(df[x] == y) & (df[self.c_n] == z)]) /len(df[df[self.c_n] == z])\n",
    "        \n",
    "        if res == 0.0:\n",
    "            return 1/(len(df[df[self.c_n] == z]) + len(df[x].unique()))\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def likelihood_for_all_(self):     \n",
    "        df = self.df\n",
    "        \n",
    "        dict1 = {}\n",
    "        for x in self.cols:\n",
    "            dict2 = {}\n",
    "            for y in df[x].unique():\n",
    "                dict3 = {}\n",
    "                for z in df[self.c_n].unique():\n",
    "                    #print('P({}=\"{}\"|{}=\"{}\") = {}'.format(x,y,self.c_n,z,self.likelihood_cal(x, y, z)))\n",
    "                    dict3[z] = self.likelihood_cal(x, y, z)\n",
    "                dict2[y] = dict3\n",
    "            dict1[x] = dict2\n",
    "        \n",
    "        self.store = dict1\n",
    "    \n",
    "    def likelihood_expr(self, class_val, expr):\n",
    "        val = 1  \n",
    "        \n",
    "        for k,v in expr:\n",
    "            try:\n",
    "                store_val = self.store[k][v][class_val]\n",
    "            except:\n",
    "                store_val = self.likelihood_cal(k,v,class_val)\n",
    "                \n",
    "            val *= store_val\n",
    "                                         \n",
    "        return val\n",
    "    \n",
    "    def prior(self, class_val):\n",
    "        df = self.df\n",
    "        return len(df[df[self.c_n] == class_val])/df.shape[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        df = self.df\n",
    "        \n",
    "        if type(X) == pd.core.series.Series:\n",
    "            values_list = [list(X.items())]\n",
    "            \n",
    "        elif type(X) == pd.core.frame.DataFrame:\n",
    "            values_list = [list(y.items()) for x,y in X.iterrows()]\n",
    "            \n",
    "        else:\n",
    "            raise TypeError('{} is not supported type'.format(type(X)))\n",
    "            \n",
    "        \n",
    "        predictions_list = []\n",
    "        for values in values_list:\n",
    "            likelihood_priors = {}\n",
    "            for class_val in df[self.c_n].unique():\n",
    "                likelihood_priors[class_val] = self.prior(class_val)*self.likelihood_expr(class_val,values)\n",
    "            #print(likelihood_priors)\n",
    "            \n",
    "            normalizing_prob = np.sum([x for x in likelihood_priors.values()])\n",
    "            probabilities = [(y/normalizing_prob,x) for x,y in likelihood_priors.items()]\n",
    "           \n",
    "            \n",
    "            if len(probabilities) == 2:\n",
    "                # For 2 Class Predictions\n",
    "                max_prob = max(probabilities)[1]\n",
    "                predictions_list.append(max_prob)\n",
    "            \n",
    "            else:\n",
    "                # For Mulit Class Predictions\n",
    "                exp_1 = [np.exp(x) for x,y in probabilities]\n",
    "                exp_2 = np.sum(exp_1)\n",
    "                softmax = exp_1/exp_2\n",
    "                #print(softmax)\n",
    "                class_names = [y for x,y in probabilities]\n",
    "                softmax_values = [(x,y) for x,y in zip(softmax,class_names)]\n",
    "                #print(softmax_values)\n",
    "                max_prob = max(softmax_values)[1]\n",
    "                predictions_list.append(max_prob)\n",
    "        \n",
    "        return predictions_list\n",
    "    \n",
    "    def accuracy_score(self, X, Y):\n",
    "        assert len(X) == len(Y), 'Given values are not equal in size'\n",
    "        \n",
    "        total_matching_values = [x == y for x,y in zip(X,Y)]\n",
    "        return (np.sum(total_matching_values)/len(total_matching_values))*100\n",
    "    \n",
    "    def calculate_confusion_matrix(self, X, Y):\n",
    "        df = self.df\n",
    "        \n",
    "        unique_class_values = df[self.c_n].unique()\n",
    "        decimal_class_values = list(range(len(unique_class_values)))\n",
    "        numerical = {x:y for x,y in zip(unique_class_values, decimal_class_values)}\n",
    "        \n",
    "        x = [numerical[x] for x in X]\n",
    "        y = [numerical[y] for y in Y]\n",
    "        \n",
    "        \n",
    "        n = len(decimal_class_values)\n",
    "        confusion_matrix = np.zeros((n,n))\n",
    "        \n",
    "        for i,j in zip(x,y):\n",
    "            if i == j:\n",
    "                confusion_matrix[i][i] += 1\n",
    "            elif i != j:\n",
    "                confusion_matrix[i][j] += 1\n",
    "        \n",
    "        return confusion_matrix\n",
    "            \n",
    "    \n",
    "    def precision_score(self, X, Y):\n",
    "        assert len(X) == len(Y), 'Given values are not equal in size'\n",
    "        \n",
    "        confusion_matrix = self.calculate_confusion_matrix(X,Y)\n",
    "        tp = confusion_matrix[0][0]\n",
    "        fp = confusion_matrix[1][0]\n",
    "        \n",
    "        return tp / (tp+fp)\n",
    "    \n",
    "    def recall_score(self, X, Y):\n",
    "        assert len(X) == len(Y), 'Given values are not equal in size'\n",
    "        \n",
    "        confusion_matrix = self.calculate_confusion_matrix(X,Y)\n",
    "        tp = confusion_matrix[0][0]\n",
    "        fn = confusion_matrix[0][1]\n",
    "        \n",
    "        return tp / (tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genx = NB(target='Category',dataframe=training_data)\n",
    "print(\"Object Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = list(testing_data.iloc[0:20,2])\n",
    "\n",
    "y_pred = genx.predict(testing_data.iloc[0:20, 0:1])\n",
    "print(y_test)\n",
    "print(y_pred)\n",
    "print('Accuracy Score -> {} %'.format(round(genx.accuracy_score(y_test,y_pred),3)))\n",
    "# print('Precison Score -> {}'.format(round(genx.precision_score(y_test,y_pred),3)))\n",
    "# print('Recall Score -> {}'.format(round(genx.recall_score(y_test,y_pred),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "HUNDRED = 100\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'sport', 'entertainment', 'politics', 'business'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF DATA POINTS IN TRAIN DATA : 38153\n",
      "NUMBER OF DATA POINTS IN TEST DATA  : 9539\n"
     ]
    }
   ],
   "source": [
    "# Dividing the data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = train_data\n",
    "y_train = train_data['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.20)\n",
    "\n",
    "print(\"NUMBER OF DATA POINTS IN TRAIN DATA :\", X_train.shape[0])\n",
    "print(\"NUMBER OF DATA POINTS IN TEST DATA  :\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Words in Train Data : 30003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_vectorizer = CountVectorizer()\n",
    "train_text_encoded = text_vectorizer.fit_transform(X_train['Text'])\n",
    "\n",
    "train_text_features = text_vectorizer.get_feature_names_out()\n",
    "train_text_feature_counts = train_text_encoded.sum(axis=0).A1\n",
    "text_feature_dict = dict(zip(list(train_text_features),train_text_feature_counts))\n",
    "\n",
    "print(\"Total Number of Unique Words in Train Data :\",len(train_text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30003\n",
      "28232\n"
     ]
    }
   ],
   "source": [
    "print(len(text_feature_dict))\n",
    "for word, frequency in list(text_feature_dict.items()):\n",
    "    if word[0] >= '0' and word[0] <= '9':\n",
    "        del text_feature_dict[word]\n",
    "\n",
    "print(len(text_feature_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "train_text_encoded = normalize(train_text_encoded, axis=0)\n",
    "test_text_encoded = text_vectorizer.transform(X_test['Text'])\n",
    "test_text_encoded = normalize(test_text_encoded, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best alpha = 1000 The train log loss is: 0.717667519168849\n",
      "For values of best alpha = 1000 The cross validation log loss is: 0.9991294152750505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000]\n",
    "HUNDRED = 100\n",
    "cv_log_error_array = []\n",
    "\n",
    "for i in alpha:\n",
    "    clf = MultinomialNB(alpha=i)\n",
    "    clf.fit(train_text_encoded, y_train)\n",
    "    \n",
    "    nb_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    nb_sig_clf.fit(train_text_encoded, y_train)\n",
    "    \n",
    "    sig_clf_probs = nb_sig_clf.predict_proba(test_text_encoded)\n",
    "    \n",
    "    cv_log_error_array.append(log_loss(y_test, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "\n",
    "best_alpha = np.argmax(cv_log_error_array)\n",
    "\n",
    "clf = MultinomialNB(alpha=alpha[best_alpha])\n",
    "clf.fit(train_text_encoded, y_train)\n",
    "\n",
    "nb_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "nb_sig_clf.fit(train_text_encoded, y_train)\n",
    "\n",
    "predict_y = nb_sig_clf.predict_proba(train_text_encoded)\n",
    "print('For values of best alpha =', alpha[best_alpha],\"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-6))\n",
    "\n",
    "predict_y = nb_sig_clf.predict_proba(test_text_encoded)\n",
    "print('For values of best alpha =', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train log loss is: 0.5736717985620965\n",
      "The cross validation log loss is: 0.7748856372408502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(train_text_encoded, y_train)\n",
    "\n",
    "nb_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "nb_sig_clf.fit(train_text_encoded, y_train)\n",
    "\n",
    "predict_y = nb_sig_clf.predict_proba(train_text_encoded)\n",
    "print(\"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-6))\n",
    "\n",
    "predict_y = nb_sig_clf.predict_proba(test_text_encoded)\n",
    "print(\"The cross validation log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted_y = nb_sig_clf.predict(test_text_encoded)\n",
    "# train_accuracy = (nb_sig_clf.score(train_text_encoded, y_train)*HUNDRED)\n",
    "cv_accuracy = (accuracy_score(predicted_y, y_test)*HUNDRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes CV Accuracy - 72.58622497117099\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes CV Accuracy -\",cv_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'category_id_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m conf_mat \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_test)\n\u001b[0;32m      2\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(conf_mat, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m----> 3\u001b[0m             xticklabels\u001b[38;5;241m=\u001b[39m\u001b[43mcategory_id_df\u001b[49m\u001b[38;5;241m.\u001b[39mCategory\u001b[38;5;241m.\u001b[39mvalues, yticklabels\u001b[38;5;241m=\u001b[39mcategory_id_df\u001b[38;5;241m.\u001b[39mCategory\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'category_id_df' is not defined"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_test)\n",
    "sns.heatmap(conf_mat, annot=True)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "666cc40e7ce6d31c1437a93e07d5e72f3187b89406f08fe2ceec683eb8bf1832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
